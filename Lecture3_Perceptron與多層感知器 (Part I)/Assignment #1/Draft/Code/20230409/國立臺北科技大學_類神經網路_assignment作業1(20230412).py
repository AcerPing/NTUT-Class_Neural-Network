# -*- coding: utf-8 -*-
"""國立臺北科技大學_類神經網路_Assignment作業1(20230412).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ec55fYau3lWtHP5frpkWJIf0rm9iEQE
"""

# 丟到模型前: pandas 
# 丟到模型時: numpy
import pandas as pd
import numpy as np
from copy import copy
iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')
iris_data = copy(iris)
iris

# 接下來要看模型Model，所以先將iris資料轉成array
iris_array = np.array(iris)
iris_array

"""第一題：要選擇怎樣的特徵"""

# 特徵選擇
# 1.) 具體、可觀察
# 2.) 可量化
# 3.) 具代表性，可區別

# 四個：無法視覺化，且沒有量化的指標代表混亂情況
# 選一個或兩個

sepal_length = iris_array[:, 0].astype(float) #'sepal length (cm)'
sepal_width = iris_array[:, 1].astype(float) #'sepal width (cm)'
petal_length = iris_array[:, 2].astype(float) #'petal length (cm)'
petal_width = iris_array[:, 3].astype(float) #'petal width (cm)'
target = iris_array[:, 4].astype(object)

import matplotlib.pyplot as plt
import seaborn as sns

# sepal length vs. sepal width
sns.scatterplot(x=sepal_length, y=sepal_width, hue=target)
plt.title('sepal length vs. sepal width')
plt.xlabel('sepal length')
plt.ylabel('sepal width')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

# sepal length vs. petal length
sns.scatterplot(x=sepal_length, y=petal_length, hue=target)
plt.title('sepal length vs. petal length')
plt.xlabel('sepal length')
plt.ylabel('petal length')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

sns.scatterplot(x=sepal_length, y=petal_width, hue=target)
plt.title('sepal length vs. petal width')
plt.xlabel('sepal length')
plt.ylabel('petal width')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

# sepal width vs. petal length
sns.scatterplot(x=sepal_width, y=petal_length, hue=target)
plt.title('sepal width vs. petal length')
plt.xlabel('sepal width')
plt.ylabel('petal length')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

# sepal width vs. petal width
sns.scatterplot(x=sepal_width, y=petal_width, hue=target)
plt.title('sepal width vs. petal width')
plt.xlabel('sepal width')
plt.ylabel('petal width')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

# petal length vs. petal width
sns.scatterplot(x=petal_length, y=petal_width, hue=target)
plt.title('petal length vs. petal width')
plt.xlabel('petal length')
plt.ylabel('petal width')
plt.legend(['setosa', 'versicolor', 'virginica'], loc='upper right')

iris['species'] = pd.factorize(iris['species'])[0]
iris

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

# 相關係數：兩個東西的相關性
# 完全正相關:1；完全負相關:-1
plt.figure(figsize=(14,10))
plt.title('Pearson Correlation', y=1.05, size=15)
sns.heatmap(iris.astype(float).corr(), linewidths=0.1, square=True, linecolor='white', annot=True, cmap="BrBG")

pd.DataFrame(iris.astype(float).corr())
# pd.DataFrame(iris.astype(float).corr()).to_excel("heatmap.xlsx")

# 選取特徵：petal length & petal width
iris_data = iris_data.drop(["sepal_length", "sepal_width"], axis=1)
iris_data

"""第二題：怎麼切分資料"""

# train_test_split
from sklearn.model_selection import train_test_split
train, test = train_test_split(iris_data, train_size=0.7, test_size = 0.2, shuffle=True, stratify=iris_data['species'])

# from sklearn.utils import resample
# train = resample(iris_data, n_samples=105, replace=False, stratify=iris_data['species'])

train

# train[train['species']=='setosa'].shape
# train[train['species']=='virginica'].shape
# train[train['species']=='versicolor'].shape

train_DataFrame = pd.DataFrame({'setosa資料數量：':[train[train['species']=='setosa'].shape[0]],
      'versicolor資料數量：':[train[train['species']=='versicolor'].shape[0]],
      'virginica資料數量：':[train[train['species']=='virginica'].shape[0]],
      }, index=['Count'])
train_DataFrame = train_DataFrame.T
train_DataFrame.columns
train_DataFrame

"""第三題：Perceptron Learning Algorithm"""

# 利用get_dummies
train = pd.get_dummies(train, columns=["species"])

# 利用轉換
# target_class = {
#     'setosa':1,
#     'virginica':-1,
#     'versicolor':-1
# }
# train['species'] = train['species'].map(target_class)
train

# 1.) 區分Setosa
train_petal_length = np.array(train)[:,0]
train_petal_width = np.array(train)[:,1]
species_setosa = np.array(train)[:,2]
# sns.scatterplot(x=train_petal_length, y=train_petal_width, hue=species_setosa)
sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_setosa')

learning_rate = 0.95
Iters = 5

Err = []

no_of_inputs = 2

# 初始權重 (petal_length,petal_width,常數) → (W1,W2,W0) → (X1,X2,X0)
# np.random.seed(10)
# weights = np.random.randn(no_of_inputs + 1)
weights = np.array([0.,0.,0.]) 
print(weights)
print("initial: " + str(weights))

inputs = np.array(train)[:,:2]
labels = np.array(train)[:,2]

# 畫圖(PLA直線)
# 定義X1和X2的範圍
x_min, x_max = 0, 10
y_min, y_max = 0, 10
# 建立X1和X2的數組
x1 = np.linspace(x_min, x_max, 100)
x2 = np.linspace(y_min, y_max, 100)
# 定義線性方程式
def eq(x1, x2):
    return (weights[0]*x1 + weights[2])/(-weights[1])

for iter_time in range(Iters): 
  err = 0
  for _input, label in zip(inputs, labels): 
    x,y = np.concatenate((_input, np.array([1.]))), label #將每一個_input都加上X0=1
    summation = np.dot(x, weights) # dot product
    if summation > 0: # the step activation function
      predicted = 1
    else: 
      predicted = 0
    weights += learning_rate * (label - predicted) * x
    # print("trained: " + str(weights))
    err += np.abs(label - predicted)
  Err.append(err)
  print('第{}次執行'.format(iter_time))
  print("trained: " + str(weights))
  
  # 繪製線性方程式圖形
  # sns.set(style="whitegrid")
  sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_setosa')
  sns.lineplot(x=x1, y=eq(x1,x2), color='red')
  plt.xlabel('petal length (cm)')
  plt.ylabel('petal width (cm)')
  plt.title('Perceptron Learning Algorithm')
  plt.show()

print("trained: " + str(weights))
print(-weights[2]/weights[0])
print(-weights[2]/weights[1])

# 繪製線性方程式圖形
# sns.set(style="whitegrid")
sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_setosa')
sns.lineplot(x=x1, y=eq(x1,x2), color='red')
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.title('Perceptron Learning Algorithm')
plt.show()

# 補畫圖

# weights= np.array([-2.47, -2.85,  5.7 ])

# x1 = np.linspace(x_min, (-weights[2])/(weights[0]), 100)

# # 繪製線性方程式圖形
# # sns.set(style="whitegrid")
# sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_setosa')
# sns.lineplot(x=x1, y=eq(x1,x2), color='red')
# plt.xlabel('petal length (cm)')
# plt.ylabel('petal width (cm)')
# plt.title('Perceptron Learning Algorithm')
# plt.show()

# print("trained: " + str(weights))
# print(-weights[2]/weights[0])
# print(-weights[2]/weights[1])

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %pylab inline
plt.plot(range(0, len(Err)),Err)
plt.xlabel('Iteration')
plt.ylabel('loss/error')

# Filter Setosa 過濾Setosa資料
train[train['species_versicolor']==0][train['species_virginica']==0].index
for number in train[train['species_versicolor']==0][train['species_virginica']==0].index: 
  train = train.drop(index=number)
# train
train = train.drop(['species_setosa'], axis= 1)
# sum(train['species_versicolor'])+sum(train['species_virginica'])

train

# 2.) 區分Versicolor
train_petal_length = np.array(train)[:,0]
train_petal_width = np.array(train)[:,1]
species_versicolor = np.array(train)[:,2]
sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_versicolor')

del sns
del plt
import seaborn as sns
import matplotlib.pyplot as plt

learning_rate = 0.001
Iters = 3000

Err = []

no_of_inputs = 2

# 初始權重 (petal_length,petal_width,常數) → (W1,W2,W0) → (X1,X2,X0)
# np.random.seed(10)
# weights = np.random.randn(no_of_inputs + 1)
weights = np.array([0.,0.,0.]) 
print(weights)
print("initial: " + str(weights))

inputs = np.array(train)[:,:2]
labels = np.array(train)[:,2]

# 畫圖(PLA直線)
# 定義X1和X2的範圍
x_min, x_max = 0, 10
y_min, y_max = 0, 10
# 建立X1和X2的數組
x1 = np.linspace(x_min, x_max, 10)
x2 = np.linspace(y_min, y_max, 10)
# 定義線性方程式
def eq(x1, x2):
    return (weights[0]*x1 + weights[2])/(-weights[1])

for iter_time in range(Iters): 
  err = 0
  for _input, label in zip(inputs, labels): 
    x,y = np.concatenate((_input, np.array([1.]))), label #將每一個_input都加上X0=1
    summation = np.dot(x, weights) # dot product
    if summation > 0: # the step activation function
      predicted = 1
    else: 
      predicted = 0
    weights += learning_rate * (label - predicted) * x

    if weights[2]!=0.0 and (weights[0])!=0.0:
      x_min, x_max = 0, ((-weights[2])/(weights[0]))
      x1 = np.linspace(x_min, x_max, 10)
      
    # print("trained: " + str(weights))
    err += np.abs(label - predicted)
  Err.append(err)

  if sum(Err[-1:-5:-1]) == 0: break #假設後續測的5個err都是0

  print('第{}次執行'.format(iter_time))
  print("trained: " + str(weights))
  
  # 繪製線性方程式圖形
  if iter_time % 5 == 0: # 每5次迭代畫一次分類線
    # sns.set(style="whitegrid")
    sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_versicolor')
    sns.lineplot(x=x1, y=eq(x1,x2), color='red')
    plt.xlabel('petal length (cm)')
    plt.ylabel('petal width (cm)')
    plt.title('Perceptron Learning Algorithm')
    plt.show()

print("trained: " + str(weights))
print(-weights[2]/weights[0])
print(-weights[2]/weights[1])

# 繪製線性方程式圖形
print('第{}次執行'.format(iter_time))
# sns.set(style="whitegrid")
sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, hue ='species_versicolor')
sns.lineplot(x=x1, y=eq(x1,x2), color='red')
plt.xlabel('petal length (cm)')
plt.ylabel('petal width (cm)')
plt.title('Perceptron Learning Algorithm')
plt.show()

print(-weights[2]/weights[0])
print(-weights[2]/weights[1])
print('第{}次執行'.format(iter_time))

# 補畫圖

# weights= np.array([-0.0016, -0.0247,  0.048])

# x1 = np.linspace(x_min, (-weights[2])/(weights[0]), 10)

# # 繪製線性方程式圖形
# # sns.set(style="whitegrid")
# sns.lmplot(x='petal_length', y='petal_width', data=train, fit_reg=False, )
# sns.lineplot(x=x1, y=eq(x1,x2), color='red')
# plt.xlabel('petal length (cm)')
# plt.ylabel('petal width (cm)')
# plt.title('Perceptron Learning Algorithm')
# plt.show()

# print("trained: " + str(weights))
# print(-weights[2]/weights[0])
# print(-weights[2]/weights[1])

"""[資料分析&機器學習] 第3.2講：線性分類-感知器(Perceptron) 介紹"""

# def sign(z):
#     if z > 0:
#         return 1
#     else:
#         return -1

# w = np.array([0.,0.,0.])
# error = 1 # error =1 主要是紀錄沒有錯誤分類的話就停止
# iterator = 0 # iterator主要是用來記錄更新了幾次

# # while error != 0:
# #     error = 0

# for i in range(len(train)):
#     x,y = np.concatenate((np.array([1.]), np.array(train.iloc[i])[:2])), np.array(train.iloc[i])[2]
#     print('y:{}'.format(y))
#     print('x:{}'.format(x))
#     # if sign(np.dot(w,x)) != y:
#     print("iterator: "+str(iterator))
#     iterator += 1
#     error += 1
#     sns.lmplot(x='petal_width', y='petal_length',data=train, fit_reg=False, hue ='species')
    
#     # 前一個Decision boundary 的法向量
#     if w[1] != 0:
#         x_last_decision_boundary = np.linspace(0,w[1])
#         y_last_decision_boundary = (w[2]/w[1])*x_last_decision_boundary
#         # plt.plot(x_last_decision_boundary, y_last_decision_boundary,'c--')
#     print('y:{}'.format(y))
#     print('x:{}'.format(x))
#     print('更新前的w:{}'.format(w))
#     w += y*x # 用來更新w     
#     print('更新後的w:{}'.format(w)) 
#     # print("x: " + str(x))            
#     # print("w: " + str(w))
#     # x向量 
#     x_vector = np.linspace(0,x[1])
#     y_vector = (x[2]/x[1])*x_vector
#     # plt.plot(x_vector, y_vector,'b')
#     # Decision boundary 的方向向量
#     x_decision_boundary = np.linspace(-0.5,7)
#     y_decision_boundary = (-w[1]/w[2])*x_decision_boundary - (w[0]/w[2])
#     plt.plot(x_decision_boundary, y_decision_boundary,'r')
#     # Decision boundary 的法向量
#     x_decision_boundary_normal_vector = np.linspace(0,w[1])
#     y_decision_boundary_normal_vector = (w[2]/w[1])*x_decision_boundary_normal_vector
#     plt.plot(x_decision_boundary_normal_vector, y_decision_boundary_normal_vector,'g')
#     plt.xlim(-0.5,7.5)
#     plt.ylim(5,-3)
#     plt.show()

# w = np.array([0.,0.,0.])
# error = 1 # error =1 主要是紀錄沒有錯誤分類的話就停止
# iterator = 0 # iterator主要是用來記錄更新了幾次
# while error != 0:
#     error = 0
#     for i in range(len(train)):
#         x,y = np.concatenate((np.array([1.]), np.array(train.iloc[i])[:2])), np.array(train.iloc[i])[2]

#         if sign(np.dot(w,x)) != y:
#             print("iterator: "+str(iterator))
#             iterator += 1
#             error += 1
#             sns.lmplot(x='petal_width', y='petal_length',data=train, fit_reg=False, hue ='species')
            
#             # 前一個Decision boundary 的法向量
#             if w[1] != 0:
#                 x_last_decision_boundary = np.linspace(0,w[1])
#                 y_last_decision_boundary = (w[2]/w[1])*x_last_decision_boundary
#                 # plt.plot(x_last_decision_boundary, y_last_decision_boundary,'c--')

#             print('y:{}'.format(y))
#             print('x:{}'.format(x))
#             print('更新前的w:{}'.format(w))
#             w += y*x # 用來更新w  
#             print('更新後')   
#             print("x: " + str(x))            
#             print("w: " + str(w))

#             # x向量 
#             x_vector = np.linspace(0,x[1])
#             y_vector = (x[2]/x[1])*x_vector
#             # plt.plot(x_vector, y_vector,'b')
#             # Decision boundary 的方向向量
#             x_decision_boundary = np.linspace(-0.5,7)
#             y_decision_boundary = (-w[1]/w[2])*x_decision_boundary - (w[0]/w[2])
#             plt.plot(x_decision_boundary, y_decision_boundary,'r')
#             # Decision boundary 的法向量
#             x_decision_boundary_normal_vector = np.linspace(0,w[1])
#             y_decision_boundary_normal_vector = (w[2]/w[1])*x_decision_boundary_normal_vector
#             plt.plot(x_decision_boundary_normal_vector, y_decision_boundary_normal_vector,'g')
#             plt.xlim(-0.5,7.5)
#             plt.ylim(-3,5)
#             plt.show()

